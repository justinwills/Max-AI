<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Networks - Modules</title>

    <link rel="stylesheet" href="../vendor/fontawesome/css/all.min.css" />
    <script src="../js/storage.js"></script>

    <link rel="stylesheet" href="../css/modules-inner.css" />
    <link rel="stylesheet" href="../css/nav.css" />

    <link rel="stylesheet" href="../vendor/sweetalert2/sweetalert2.min.css" />
    <script src="../vendor/sweetalert2/sweetalert2.all.min.js"></script>

    <script src="../vendor/jquery/jquery-3.6.0.min.js"></script>
  </head>
  <body>
    <nav>
      <div class="nav-container">
        <a href="../home.html" aria-label="Go to Home">
          <img src="../img/logo.svg" class="logo" alt="MaxAI" />
        </a>

        <div class="nav-links">
          <a href="../home.html" class="nav-link">Home</a>
          <a href="../modules.html" class="nav-link active">Modules</a>
          <a href="../flashcard.html" class="nav-link">Flashcard</a>
          <a href="../quiz.html" class="nav-link">Quiz</a>
          <a href="../about.html" class="nav-link">About</a>
        </div>

        <div class="nav-actions">
          <div class="profile-menu">
            <button
              type="button"
              class="profile-btn"
              aria-label="Profile"
              aria-expanded="false"
            >
              <i class="fas fa-user"></i>
            </button>
            <div class="menu">
              <a href="../userprofile.html"
                ><i class="fas fa-user-circle"></i> Profile</a
              >
              <a href="../feedback.html"
                ><i class="fas fa-comment-dots"></i> Feedback</a
              >
              <a href="../login.html?logout=1"
                ><i class="fas fa-right-from-bracket"></i> Logout</a
              >
            </div>
          </div>
        </div>

        <div
          class="mobile-menu-btn"
          aria-label="Open menu"
          role="button"
          tabindex="0"
        >
          <i class="fas fa-bars"></i>
        </div>
      </div>
    </nav>
    <script src="../js/nav.js"></script>

    <section id="about-home">
      <div class="hero-content">
        <div class="hero-badge">
          <i class="fas fa-robot"></i>
          <span>Specialized</span>
        </div>
        <h1>Neural Networks</h1>
        <p class="hero-subtitle">
          Explore neurons, nonlinear architectures, training algorithms, and
          modern network designs from MLPs to Transformers.
        </p>
        <div class="hero-stats">
          <div class="stat-item">
            <i class="fas fa-clock"></i>
            <span>3-5 hours</span>
          </div>
          <div class="stat-item">
            <i class="fas fa-signal"></i>
            <span>Specialized</span>
          </div>
          <div class="stat-item">
            <i class="fas fa-users"></i>
            <span>Free</span>
          </div>
        </div>
      </div>
      <div class="hero-visual">
        <div class="floating-card card-1">
          <i class="fas fa-bolt"></i>
          <span>Transformers</span>
        </div>
        <div class="floating-card card-2">
          <i class="fas fa-project-diagram"></i>
          <span>CNN</span>
        </div>
        <div class="floating-card card-3">
          <i class="fas fa-stream"></i>
          <span>RNN</span>
        </div>
      </div>
    </section>

    <section id="course-inner">
      <div class="overview">
        <div class="course-panel">
          <div class="course-header">
            <img
              class="course-img"
              src="../img/neural-networks.png"
              alt="Neural Networks"
            />
            <div class="course-overlay">
              <div class="play-button">
                <i class="fas fa-play"></i>
              </div>
            </div>
          </div>

          <div class="course-info">
            <div class="course-meta">
              <div class="meta-item">
                <i class="fas fa-calendar"></i>
                <span>Last updated: Sep 2025</span>
              </div>
            </div>

            <div class="callout info">
              <i class="fas fa-lightbulb"></i>
              <div>
                <strong>Pro Tip:</strong> Complete one lesson per day for
                optimal retention. Each lesson builds on the previous one.
              </div>
            </div>
            <div class="progress-wrap">
              <div
                id="progress-bar"
                class="progress-bar"
                role="progressbar"
                aria-valuemin="0"
                aria-valuenow="0"
                aria-valuemax="100"
              >
                <div
                  id="progress-fill"
                  class="progress-fill"
                  style="width: 0%"
                ></div>
                <div id="progress-label" class="progress-label">
                  0% Completed
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="content-sections">
          <!-- Learning Objectives ---->
          <section class="content-section" id="objectives">
            <div class="section-header">
              <div class="section-icon">
                <i class="fas fa-bullseye"></i>
              </div>
              <h3>Learning Objectives</h3>
            </div>
            <div class="objectives-grid">
              <div class="objective-item">
                <i class="fas fa-check-circle"></i>
                <span>Explain neurons, layers, and activation functions</span>
              </div>
              <div class="objective-item">
                <i class="fas fa-check-circle"></i>
                <span>Implement forward & backward passes for simple MLPs</span>
              </div>
              <div class="objective-item">
                <i class="fas fa-check-circle"></i>
                <span
                  >Choose losses/optimizers and train without
                  exploding/vanishing gradients</span
                >
              </div>
              <div class="objective-item">
                <i class="fas fa-check-circle"></i>
                <span
                  >Regularize to reduce overfitting and evaluate with the right
                  metrics</span
                >
              </div>
              <div class="objective-item">
                <i class="fas fa-check-circle"></i>
                <span
                  >Recognize when to use CNNs vs. sequence models
                  (RNNs/Transformers)</span
                >
              </div>
            </div>
          </section>

          <section class="content-section" id="prerequisites">
            <div class="section-header">
              <div class="section-icon">
                <i class="fas fa-list-check"></i>
              </div>
              <h3>Prerequisites</h3>
            </div>
            <p>
              Recommended: complete the previous module
              <a class="prereq-link" href="./deep-learning.html"
                ><i class="fas fa-book-open"></i> Deep Learning</a
              >
              to ensure you're comfortable with deep learning concepts and
              terminology. You should know basic Python and a little linear
              algebra/calculus.
            </p>
            <div class="tags">
              <span class="tag">Completed Deep Learning</span>
              <span class="tag">Advanced Math</span>
              <span class="tag">ML Basics</span>
            </div>
          </section>

          <div class="lessons-container">
            <h3 class="lessons-title">Course Lessons</h3>
            <!-- Course Lessons --->
            <div class="lesson-item" data-lesson="1">
              <div class="lesson-header">
                <div class="lesson-number">01</div>
                <div class="lesson-content">
                  <h4>Neurons & Perceptron</h4>
                  <p>
                    Learn how neurons combine inputs to form simple linear
                    decision boundaries.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 20 min</span>
                  <span><i class="fas fa-video"></i> Video</span>
                </div>
                <div class="lesson-preview">
                  <iframe
                    width="560"
                    height="315"
                    src="https://www.youtube.com/embed/ntKn5TPHHAk"
                    title="Perceptron Explained Visually"
                    frameborder="0"
                    allowfullscreen
                  ></iframe>
                  <br />
                  <p>
                    A neuron is the basic building block of neural networks: it
                    multiplies inputs by weights, adds a bias, and passes the
                    result through an activation function to decide whether it
                    â€œfires.â€ The perceptron, the earliest version of this
                    idea, can only solve linearly separable problems â€” like
                    drawing a straight line to divide categories on a chart. You
                    can think of it like a light switch with a dial: it only
                    flips on when the combined input is strong enough. While
                    limited, this simple mechanism is important because it laid
                    the groundwork for all modern neural networks and gave us
                    the first glimpse of how machines can make decisions from
                    numbers.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> neuron, weight, bias,
                    activation, perceptron, linear separability
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="2">
              <div class="lesson-header">
                <div class="lesson-number">02</div>
                <div class="lesson-content">
                  <h4>Linear Neural Networks</h4>
                  <p>
                    Learn how linear neural networks process inputs with
                    weighted sums, why they are limited to linearly separable
                    problems
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 15 min</span>
                  <span><i class="fas fa-image"></i> Image</span>
                </div>
                <div class="lesson-preview">
                  <img src="../img/nn-1.png" alt="Lesson 1 Preview" />
                  <p>
                    Linear neural networks stack layers that perform only linear
                    operations, but no matter how many are added, the result is
                    still just one linear transformation. This makes them
                    powerful for simple problems but incapable of modeling
                    curved or complex decision boundaries. Imagine stacking
                    magnifying glasses on top of each other: you can enlarge an
                    image, but you never fundamentally change it. Recognizing
                    this limitation underscores why nonlinearity is essential
                    â€” it is the key that allows neural networks to go beyond
                    traditional linear models and tackle real-world complexity.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> linear layer, affine, rank,
                    expressivity
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="3">
              <div class="lesson-header">
                <div class="lesson-number">03</div>
                <div class="lesson-content">
                  <h4>Multilayer Perceptrons (MLPs) & Nonlinearity</h4>
                  <p>
                    See how nonlinear activations make MLPs capable of modeling
                    complex patterns.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 25 min</span>
                  <span><i class="fas fa-image"></i> Image</span>
                </div>
                <div class="lesson-preview">
                  <img src="../img/nn-2.png" alt="Lesson 2 Preview" />
                  <p>
                    Multilayer perceptrons add hidden layers and nonlinear
                    activation functions like ReLU or Tanh, breaking the â€œjust
                    one linear map barrier. This unlocks the ability to
                    approximate highly complex functions, as proven by the
                    universal approximation theorem. Think of it like a factory
                    assembly line: each stage makes small changes, but
                    nonlinearities are where the magic happens â€” bending,
                    shaping, or painting the product into something new. This
                    combination makes MLPs the prototype for most modern neural
                    networks, showing how stacking simple parts with
                    nonlinearity creates surprising power.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> MLP, hidden layer, ReLU,
                    SiLU/Swish
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="4">
              <div class="lesson-header">
                <div class="lesson-number">04</div>
                <div class="lesson-content">
                  <h4>Losses, Backprop & Gradient Descent</h4>
                  <p>
                    Learn how backpropagation and gradient descent train
                    networks by reducing loss.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 30 min</span>
                  <span><i class="fas fa-video"></i> Video</span>
                </div>
                <div class="lesson-preview">
                  <iframe
                    width="560"
                    height="315"
                    src="https://www.youtube.com/embed/tIeHLnjs5U8"
                    title="Backpropagation Explained"
                    frameborder="0"
                    allowfullscreen
                  ></iframe>

                  <br />
                  <p>
                    Training a network means reducing the gap between
                    predictions and true labels, quantified by a loss function.
                    Backpropagation efficiently computes how much each parameter
                    contributes to the loss using the chain rule, and gradient
                    descent updates the parameters step by step to minimize it.
                    Itâ€™s like hiking downhill in thick fog: the loss is your
                    altitude, the gradient is the slope under your feet, and
                    gradient descent is carefully stepping toward the valley.
                    This method is what made large-scale training possible,
                    powering the modern era of deep learning.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> loss function, cross-entropy,
                    backpropagation, SGD, mini-batch
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="5">
              <div class="lesson-header">
                <div class="lesson-number">05</div>
                <div class="lesson-content">
                  <h4>Regularization & Generalization</h4>
                  <p>
                    Discover how regularization methods reduce overfitting and
                    improve generalization.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 12 min</span>
                  <span><i class="fas fa-file-text"></i> Notes</span>
                </div>
                <div class="lesson-preview">
                  <p>
                    Neural networks often overfit â€” memorizing training data
                    but failing on new examples. Regularization solves this by
                    encouraging robustness. Weight decay discourages extreme
                    weights, dropout randomly silences neurons so others must
                    adapt, and data augmentation creates variations of inputs to
                    promote flexibility. Itâ€™s similar to studying for an exam:
                    memorizing past questions may fail you on new ones, but
                    practicing with varied exercises ensures true understanding.
                    These techniques are vital because without them, models
                    would collapse when facing slightly different real-world
                    data.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> overfitting, underfitting,
                    weight decay, dropout, early stopping, augmentation
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="6">
              <div class="lesson-header">
                <div class="lesson-number">06</div>
                <div class="lesson-content">
                  <h4>Initialization, Normalization & Optimizers</h4>
                  <p>
                    Explore techniques that stabilize training and speed
                    convergence in deep networks.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 25 min</span>
                  <span><i class="fas fa-image"></i> Image</span>
                </div>
                <div class="lesson-preview">
                  <img src="../img/nn-3.svg" alt="Lesson 3 Preview" />
                  <p>
                    Successful training depends on careful setup and tuning.
                    Poor weight initialization can cause gradients to vanish or
                    explode, but methods like Xavier or He scaling balance this.
                    Normalization layers such as BatchNorm stabilize
                    activations, making learning smoother. Optimizers like Adam
                    add momentum and adaptive step sizes, accelerating progress.
                    Imagine preparing for a marathon: initialization is starting
                    with the right energy, normalization is keeping your pace
                    steady, and optimizers are like wearing better shoes with a
                    smart race strategy. Together, these techniques make
                    training large networks both feasible and efficient.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> Xavier/He init, BatchNorm,
                    LayerNorm, Adam, AdamW, learning-rate schedule, warmup
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="7">
              <div class="lesson-header">
                <div class="lesson-number">07</div>
                <div class="lesson-content">
                  <h4>Convolutional Neural Networks (CNNs) for Vision</h4>
                  <p>
                    Learn how CNNs extract spatial features for image and vision
                    tasks.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 25 min</span>
                  <span><i class="fas fa-file-text"></i> Video</span>
                </div>
                <div class="lesson-preview">
                  <iframe
                    width="560"
                    height="315"
                    src="https://www.youtube.com/embed/YRhxdVk_sIs"
                    title="CNNs Explained"
                    frameborder="0"
                    allowfullscreen
                  ></iframe>
                  <br />
                  <p>
                    CNNs specialize in images by using filters that scan small
                    local regions and share weights across the input, making
                    them efficient at detecting edges, textures, and shapes.
                    Pooling layers condense information while preserving key
                    features, building up hierarchical representations from
                    low-level details to high-level concepts. Think of CNNs like
                    photographers with zoom lenses: they capture fine details
                    first, then assemble them into a full picture. This design
                    revolutionized computer vision, enabling breakthroughs in
                    tasks like object recognition, medical imaging, and
                    autonomous driving.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> convolution, kernel, stride,
                    padding, pooling, feature map
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="8">
              <div class="lesson-header">
                <div class="lesson-number">08</div>
                <div class="lesson-content">
                  <h4>Sequences: RNNs vs. Transformers</h4>
                  <p>
                    Compare RNNsâ€™ stepwise processing with Transformersâ€™
                    self-attention mechanism.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 1 hour 20 min</span>
                  <span><i class="fas fa-file-text"></i> Video</span>
                </div>
                <div class="lesson-preview">
                  <iframe
                    width="560"
                    height="315"
                    src="https://www.youtube.com/embed/KJtZARuO3JY"
                    title="Transformers & Attention"
                    frameborder="0"
                    allowfullscreen
                  ></iframe>
                  <br />
                  <p>
                    Sequential data like text or speech requires models that
                    handle order and context. RNNs do this by passing hidden
                    states step by step, but struggle with long sequences due to
                    vanishing gradients. LSTMs and GRUs alleviate the problem,
                    yet still have limits. Transformers solved it by using
                    self-attention, allowing every element to directly relate to
                    every other. Itâ€™s like the difference between reading a
                    book word by word and scanning the whole page at once while
                    highlighting important connections. This leap in capability
                    explains why Transformers now power everything from chatbots
                    to large-scale generative AI.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> RNN, LSTM/GRU, attention,
                    self-attention, positional encoding, encoder/decoder
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="9">
              <div class="lesson-header">
                <div class="lesson-number">09</div>
                <div class="lesson-content">
                  <h4>Neural Network Mini Lab</h4>
                  <p>Try code on your own!</p>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 30 min</span>
                  <span><i class="fas fa-file-text"></i> Notes</span>
                </div>
                <div class="lesson-preview">
                  <p>
                    See linear limits: choose Linear dataset â†’ small noise â†’
                    watch the boundary form quickly with a few hidden units.
                    Solve XOR: switch to XOR, try tanh with 8â€“12 hidden units
                    and lr â‰ˆ 0.03â€“0.1. Notice how nonlinearity + hidden
                    units enable learning. Explore ReLU vs tanh: on Circles,
                    compare ReLU vs tanh (ReLU often needs a bit more hidden
                    units). Regularization: add small L2 (e.g., 0.001) and see
                    smoother, less overfit boundaries at high noise.
                    Reproducibility: tweak the seed; use Save/Load to keep a
                    session; Export the trained model as JSON and Import it
                    later.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> linear limits, relu, tanh,
                    regularization
                  </div>
                </div>
              </div>
            </div>

            <!-- Module summary -->
            <div class="lesson-item" data-lesson="10">
              <div class="lesson-header">
                <div class="lesson-number">10</div>
                <div class="lesson-content">
                  <h4>Module Summary</h4>
                  <p>
                    Nonlinearity is what gives neural networks their expressive
                    power, while depth allows them to build hierarchical feature
                    representations. At the core of training, backpropagation
                    together with SGD-family optimizers serves as the workhorse,
                    with stability depending heavily on proper initialization,
                    normalization, and learning rate choices. To ensure
                    real-world performance, networks must generalize beyond the
                    training data, which requires regularization techniques and
                    disciplined evaluation. In practice, CNNs remain the
                    dominant choice for image-based tasks, whereas sequence
                    modeling is increasingly driven by attention-based
                    architectures such as Transformers.
                  </p>
                  <p>
                    Next steps: explore the Natural Language Processing and AI
                    Ethics modules, try a small project.
                  </p>
                </div>
              </div>
            </div>
          </div>

          <section class="content-section" id="resources">
            <div class="section-header">
              <div class="section-icon">
                <i class="fas fa-brain"></i>
              </div>
              <h3>Further Resources</h3>
            </div>
            <div class="resources-grid">
              <div class="resource-card">
                <div class="resource-icon">
                  <i class="fas fa-book"></i>
                </div>
                <h4>Neural Networks and Deep Learning</h4>
                <p>
                  Michael Nielsenâ€™s classic free online book for beginners.
                </p>
                <a
                  href="http://neuralnetworksanddeeplearning.com/"
                  class="resource-link"
                >
                  Learn More <i class="fas fa-arrow-right"></i>
                </a>
              </div>
              <div class="resource-card">
                <div class="resource-icon">
                  <i class="fas fa-laptop-code"></i>
                </div>
                <h4>Deep Learning Specialization</h4>
                <p>
                  Andrew Ngâ€™s Coursera program focusing on neural networks and
                  deep learning.
                </p>
                <a
                  href="https://www.coursera.org/specializations/deep-learning"
                  class="resource-link"
                >
                  Learn More <i class="fas fa-arrow-right"></i>
                </a>
              </div>
              <div class="resource-card">
                <div class="resource-icon">
                  <i class="fas fa-database"></i>
                </div>
                <h4>TensorFlow Playground</h4>
                <p>
                  Interactive tool to visualize and experiment with neural
                  networks in the browser.
                </p>
                <a
                  href="https://playground.tensorflow.org/"
                  class="resource-link"
                >
                  Try It <i class="fas fa-arrow-right"></i>
                </a>
              </div>
            </div>
          </section>
        </div>
      </div>

      <aside class="enroll toc">
        <div class="toc-downloads">
          <h4>Course Materials</h4>
          <div class="download-item">
            <i class="fa-solid fa-file-lines"></i>
            <span>Summary notes (PDF)</span>
            <a
              class="download-btn download-notes"
              href="../materials/neural-networks-summary.pdf"
              download
              aria-label="Download summary notes PDF"
              >Download</a
            >
          </div>
          <div class="download-item">
            <i class="fa-solid fa-file-code"></i>
            <span>Mini Lab code (HTML)</span>
            <a
              class="download-btn"
              href="../materials/neural-networks-sample.html"
              download
              aria-label="Download Mini Lab code (HTML)"
            >
              Download
            </a>
          </div>
        </div>

        <div class="toc-info">
          <div class="info-item">
            <i class="fa-regular fa-clock"></i>
            <span>Estimated time: ~4 hours</span>
          </div>
        </div>
      </aside>
    </section>
    <footer>
      <div class="footer-bottom">
        <p>
          &copy; 2025 MaxAI |
          <a href="mailto:rachellwanggg@gmail.com">andrianateam.com</a>
        </p>
      </div>
    </footer>

    <script src="neural-networks.js"></script>
  </body>
</html>
