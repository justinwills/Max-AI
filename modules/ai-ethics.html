<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Ethics & Future - Modules</title>

    <link rel="stylesheet" href="../vendor/fontawesome/css/all.min.css" />
    <script src="../js/storage.js"></script>

    <link rel="stylesheet" href="../css/modules-inner.css" />

    <link rel="stylesheet" href="../vendor/sweetalert2/sweetalert2.min.css" />
    <script src="../vendor/sweetalert2/sweetalert2.all.min.js"></script>

    <script
      src="https://code.jquery.com/jquery-3.6.0.min.js"
      integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4="
      crossorigin="anonymous"
    ></script>
  </head>
  <body>
    <nav>
        <img src="../img/logo.svg" class="logo" alt="MaxAI" />
      </a>

      <div class="navigation">
        <ul>
          <li><a href="../home.html">Home</a></li>
          <li><a href="../modules.html" class="active">Modules</a></li>
          <li><a href="../flashcard.html">Flashcard</a></li>
          <li><a href="../quiz.html">Quiz</a></li>
          <li><a href="../about.html">About</a></li>
        </ul>
        <div class="mobile-menu-btn" aria-label="Open menu" role="button" tabindex="0">
          <i class="fas fa-bars"></i>
        </div>
      </div>

      <div class="profile-menu">
        <button
          type="button"
          class="profile-btn"
          aria-label="Profile"
          aria-expanded="false"
        >
          <i class="fas fa-user"></i>
        </button>
        <div class="menu">
          <a href="../userprofile.html"
            ><i class="fas fa-user-circle"></i> Profile</a
          >
          <a href="../login.html?logout=1"
            ><i class="fas fa-right-from-bracket"></i> Logout</a
          >
        </div>
      </div>
    </nav>
    <script src="../js/nav.js"></script>
    <script>
      // Mobile menu toggle for inner modules page
      (function(){
        const burger = document.querySelector('.mobile-menu-btn');
        const menuList = document.querySelector('nav .navigation ul');
        if (burger && menuList) {
          burger.addEventListener('click', function(){
            menuList.classList.toggle('active');
          });
          // Close menu when a link is tapped (mobile)
          menuList.querySelectorAll('a').forEach(a => a.addEventListener('click', () => {
            menuList.classList.remove('active');
          }));
        }
      })();
    </script>

    <section id="about-home">
      <div class="hero-content">
        <div class="hero-badge">
          <i class="fas fa-robot"></i>
          <span>All Levels</span>
        </div>
        <h1>AI Ethics & Future</h1>
        <p class="hero-subtitle">
               Examine the ethical challenges, societal impacts, and the future direction of artificial intelligence.
        </p>
        <div class="hero-stats">
          <div class="stat-item">
            <i class="fas fa-clock"></i>
            <span>1-2 hours</span>
          </div>
          <div class="stat-item">
            <i class="fas fa-signal"></i>
            <span>All Levels</span>
          </div>
          <div class="stat-item">
            <i class="fas fa-users"></i>
            <span>Free</span>
          </div>
        </div>
      </div>
      <div class="hero-visual">
        <div class="floating-card card-1">
          <i class="fas fa-lock"></i>
          <span>Privacy</span>
        </div>
        <div class="floating-card card-2">
          <i class="fas fa-bullseye"></i>
          <span>Trend</span>
        </div>
        <div class="floating-card card-3">
          <i class="fas fa-chart-line"></i>
          <span>Data</span>
        </div>
      </div>
    </section>

    <section id="course-inner">
      <div class="overview">
        <div class="course-panel">
          <div class="course-header">
            <img
              class="course-img"
              src="../img/ai-ethics.png"
              alt="AI Ethics & Future"
            />
            <div class="course-overlay">
              <div class="play-button">
                <i class="fas fa-play"></i>
              </div>
            </div>
          </div>

          <div class="course-info">
            <div class="course-meta">
              <div class="meta-item">
                <i class="fas fa-calendar"></i>
                <span>Last updated: Aug 2025</span>
              </div>
            </div>

            <div class="callout info">
              <i class="fas fa-lightbulb"></i>
              <div>
                <strong>Pro Tip:</strong> Complete one lesson per day for
                optimal retention. Each lesson builds on the previous one.
              </div>
            </div>
            <div class="progress-wrap">
              <div
                id="progress-bar"
                class="progress-bar"
                role="progressbar"
                aria-valuemin="0"
                aria-valuenow="0"
                aria-valuemax="100"
              >
                <div
                  id="progress-fill"
                  class="progress-fill"
                  style="width: 0%"
                ></div>
                <div id="progress-label" class="progress-label">
                  0% Completed
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="content-sections">
        <!-- Learning Objectives ---->
        <section class="content-section" id="objectives">
        <div class="section-header">
            <div class="section-icon">
            <i class="fas fa-bullseye"></i>
            </div>
            <h3>Learning Objectives</h3>
        </div>
        <div class="objectives-grid">
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Identify how bias enters AI systems and apply fairness metrics & mitigations</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Explain privacy principles such as data minimization, differential privacy, and federated learning</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Document and audit models with transparency tools like Model Cards and Datasheets</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Use explainability responsibly and define escalation/remediation paths for accountability</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Apply safety practices: red-teaming, alignment, runtime safeguards, and human-in-the-loop</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Discuss emerging trends: Retrieval-Augmented Generation, edge deployment, and agentic workflows</span>
            </div>
            <div class="objective-item">
            <i class="fas fa-check-circle"></i>
            <span>Reflect on responsible AI practices that balance innovation, ethics, and real-world duty of care</span>
            </div>
        </div>
        </section>

        <section class="content-section" id="prerequisites">
        <div class="section-header">
            <div class="section-icon">
            <i class="fas fa-list-check"></i>
            </div>
            <h3>Prerequisites</h3>
        </div>
        <p>
            Curiosity about how AI affects people and society. No coding required—focus is on concepts, risks, and governance. 
            Optional: familiarity with basic AI/ML ideas helps but is not necessary.
        </p>
        <div class="tags">
            <span class="tag">No coding required</span>
            <span class="tag">Beginner friendly</span>
            <span class="tag">Focus on ethics & governance</span>
        </div>
        </section>

          <div class="lessons-container">
            <h3 class="lessons-title">Course Lessons</h3>
            <!-- Course Lessons --->
            <div class="lesson-item" data-lesson="1">
              <div class="lesson-header">
                <div class="lesson-number">01</div>
                <div class="lesson-content">
                  <h4>Fairness from Data to Decisions</h4>
                  <p>
                    Measuring and mitigating bias in real workflows
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 10 min</span>
                  <span><i class="fas fa-video"></i> Video</span>
                </div>
                <div class="lesson-preview">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/yN7ypxC7838" 
title="All Machine Learning Models Explained in 5 Minutes" frameborder="0" allowfullscreen></iframe>
                    <br>
                  <p>
                    Bias creeps in through datasets (sampling, labeling), features (proxies for protected attributes), and 
decision thresholds (different error costs across groups). A practical workflow is: define harm and 
stakeholders → choose fairness metrics aligned to context → run disaggregated evaluation (by 
subgroup) → mitigate via data balancing/reweighting, algorithmic changes (e.g., constraints for equalized odds), or post-processing (group-aware thresholds) → monitor drift over time. Remember that metrics trade off: demographic parity equalizes positive rates, while equalized odds equalizes error rates—pick based on the real-world duty of care (e.g., screening vs. adjudication). Document decisions and residual risks
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> demographic parity, equalized odds, calibration, subgroup analysis, post-processing
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="2">
              <div class="lesson-header">
                <div class="lesson-number">02</div>
                <div class="lesson-content">
                  <h4>Privacy & Data Governance by Design</h4>
                  <p>
                    Protecting people while preserving utility
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 6 min</span>
                  <span><i class="fas fa-file-text"></i> Notes</span>
                </div>
                <div class="lesson-preview">
                  <p>
                    Build around data minimization (collect only what you need), purpose limitation (no surprise uses), and secure lifecycle (ingestion → storage → access → deletion). Technical safeguards include differential 
privacy (adding calibrated noise so single records have limited influence), federated learning (train where data lives), and secure aggregation or confidential computing to reduce exposure. Beware 
“anonymous” data—linkage attacks can re-identify rows. Track lineage and consent; log who accessed what and why. When generating synthetic data, validate that it doesn’t memorize individuals (membership inference tests). Make privacy impact assessments routine, not exceptional.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> differential privacy (ε), federated learning, secure aggregation, data minimization, lineage
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="3">
              <div class="lesson-header">
                <div class="lesson-number">03</div>
                <div class="lesson-content">
                  <h4>
                    Transparency, Explainability & Accountability
                  </h4>
                  <p>
                    Ensure ongoing transparency and accountability by documenting models and datasets, maintaining decision logs, using explainability responsibly, and defining clear escalation and remediation paths.
                  </p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 10 min</span>
                  <span><i class="fas fa-file-text"></i> Video</span>
                </div>
                <div class="lesson-preview">    
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/5e0FeCWDwM4" 
title="Fairness from Data to Decisions" frameborder="0" allowfullscreen></iframe>
                    <br>
                    <p>
Transparency is more than a one-time disclosure. Publish Model Cards (intended use, training data 
sources, metrics by subgroup, known limitations) and Datasheets for Datasets (collection process, 
consent, licensing, caveats). Maintain decision logs (inputs, versioned model, thresholds) for auditability 
and incident response. Use explainability responsibly: local methods (e.g., SHAP/LIME) can aid 
debugging, but don’t over-promise causal truth; pair explanations with user education and uncertainty 
ranges. Define escalation paths: who pauses a model, how rollbacks work, and what remediation looks 
like for affected users
                    </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> model card, datasheet, audit trail, SHAP/LIME, uncertainty disclosure
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="4">
              <div class="lesson-header">
                <div class="lesson-number">04</div>
                <div class="lesson-content">
                  <h4>Safety, Alignment & Misuse Prevention</h4>
                  <p>From red-teaming to runtime safeguards</p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 7 min</span>
                  <span><i class="fas fa-file-text"></i> Notes</span>
                </div>
                <div class="lesson-preview">
                  <p>
                    Operational safety blends pre-deployment testing with live defenses. Red-team models to probe for 
harmful content, data exfiltration, jailbreaks, prompt injection, or privacy leaks. Align models with policy 
via instruction tuning and refusal/deflection strategies; layer content filters, rate limiting, and abuse 
detection in production. For connected systems and agents, constrain tools (principle of least privilege), 
validate outputs (grounding/verification), and add human-in-the-loop for high-impact actions. 
Continuously monitor for drift and incidents; create a lightweight Safety Review checklist for every 
change (new data, new prompt, new integration).
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> red-teaming, alignment, content filtering, prompt injection, least privilege.
                  </div>
                </div>
              </div>
            </div>

            <div class="lesson-item" data-lesson="5">
              <div class="lesson-header">
                <div class="lesson-number">05</div>
                <div class="lesson-content">
                  <h4>The Near Future of AI—Trends You Can Use</h4>
                  <p>What to build, how to evaluate, and where risks shif</p>
                </div>
                <div class="lesson-status">
                  <i class="fas fa-lock-open"></i>
                </div>
              </div>
              <div class="lesson-details">
                <div class="lesson-meta">
                  <span><i class="fas fa-clock"></i> 12 min</span>
                  <span><i class="fas fa-file-text"></i> Notes</span>
                </div>
                <div class="lesson-preview">
                  <p>
                   Three impactful shifts: (1) Retrieval-Augmented Generation (RAG) to ground outputs in your sources, 
reducing hallucinations and enabling citations; invest in document chunking, metadata, and evaluation 
beyond string-match. (2) On-device/edge models for privacy, lower latency, and resilience; plan for 
model quantization and fallback paths. (3) Agentic workflows that chain tools and tasks; require 
guardrails, state inspection, and sandboxed execution. Expect stronger sustainability pressures—
optimize with distillation, quantization, caching, and scheduled batch jobs. Evaluation must evolve: add 
task-specific checklists, human ratings, and safety benchmarks alongside accuracy.
                  </p>
                  <div class="callout neutral">
                    <strong>Key terms:</strong> RAG, grounding, quantization, distillation, agent sandboxing
                  </div>
                </div>
              </div>
            </div>


            <!-- Module summary -->
            <div class="lesson-item" data-lesson="6">
              <div class="lesson-header">
                <div class="lesson-number">06</div>
                <div class="lesson-content">
                  <h4>Module Summary</h4>
                  <p>
                    You can now explain what AI is, how the Turing Test relates
                    to intelligence, and how AI evolved from expert systems to
                    modern deep learning. You can distinguish ANI vs. AGI vs.
                    ASI, structure data into features and labels with proper
                    splits, pick between supervised/unsupervised/RL, evaluate
                    models with precision/recall/F1/ROC‑AUC, mitigate
                    overfitting, and reason about fairness, privacy and
                    transparency.
                  </p>
                  <p>
                    Next steps: explore the ML and Deep Learning modules, try a
                    small project (e.g., a classifier on tabular data), and
                    practice communicating model limitations responsibly.
                  </p>
                </div>
              </div>
            </div>
          </div>

          <section class="content-section" id="resources">
            <div class="section-header">
              <div class="section-icon">
                <i class="fas fa-book-open"></i>
              </div>
              <h3>Further Resources</h3>
            </div>
            <div class="resources-grid">
              <div class="resource-card">
                <div class="resource-icon">
                  <i class="fas fa-book"></i>
                </div>
                <h4>AI For Everyone</h4>
                <p>Andrew Ng's comprehensive overview course</p>
                <a
                  href="https://www.deeplearning.ai/courses/ai-for-everyone/"
                  class="resource-link"
                  >Learn More <i class="fas fa-arrow-right"></i
                ></a>
              </div>
              <div class="resource-card">
                <div class="resource-icon">
                  <i class="fas fa-code"></i>
                </div>
                <h4>scikit-learn</h4>
                <p>Beginner‑friendly machine learning API</p>
                <a
                  href="https://scikit-learn.org/stable/index.html"
                  class="resource-link"
                  >Learn More <i class="fas fa-arrow-right"></i
                ></a>
              </div>
            </div>
          </section>
        </div>
      </div>

      <aside class="enroll toc">
        <div class="toc-downloads">
          <h4>Course Materials</h4>
          <div class="download-item">
            <i class="fa-solid fa-file-lines"></i>
            <span>Summary notes (PDF)</span>
            <a
              class="download-btn download-notes"
              href="../materials/ai-foundations-summary.pdf"
              download
              aria-label="Download summary notes PDF"
              >Download</a
            >
          </div>

        </div>

        <div class="toc-info">
          <div class="info-item">
            <i class="fa-regular fa-clock"></i>
            <span>Estimated time: ~1 hour</span>
          </div>
        </div>
      </aside>
    </section>
    <footer>
      <div class="footer-bottom">
        <p>
          &copy; 2025 MaxAI |
          <a href="mailto:rachellwanggg@gmail.com">andrianateam.com</a>
        </p>
      </div>
    </footer>

    <script src="ai-ethics.js"></script>
  </body>
</html>
